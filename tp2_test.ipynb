{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', low_memory=False)\n",
    "test = pd.read_csv('./data/test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = {'subscription_type':  'sub_type',\n",
    "           'start_station_name': 'ss_name',\n",
    "           'end_station_name':   'es_name',\n",
    "           'start_station_id':   'sid',\n",
    "           'end_station_id':     'es_id',\n",
    "           'start_date':         's_date',\n",
    "           'end_date':           'e_date',\n",
    "           'zip_code':           'z_code',\n",
    "           'bike_id':            'b_id'}\n",
    "\n",
    "train.rename(columns=nombres, inplace=True)\n",
    "train.drop(['ss_name', 'es_name', 'z_code', 'id','e_date','es_id'], axis=1, inplace=True)\n",
    "train.rename(columns={'sid':'id'}, inplace=True)\n",
    "train['s_date'] = pd.to_datetime(train['s_date'], format='%m/%d/%Y %H:%M')\n",
    "train['year'] = train['s_date'].map(lambda x:x.year)\n",
    "train['month'] = train['s_date'].map(lambda x:x.month)\n",
    "train['day'] = train['s_date'].map(lambda x:x.day)\n",
    "train['weekday'] = train['s_date'].map(lambda x: x.weekday())\n",
    "train['hour'] = train['s_date'].map(lambda x:x.hour)\n",
    "train['minute'] = train['s_date'].map(lambda x:x.minute)\n",
    "train['date'] = pd.DatetimeIndex(train['s_date']).normalize()\n",
    "train = train.merge(pd.get_dummies(train.sub_type), left_index=True, right_index=True)\n",
    "train.drop('sub_type', axis=1, inplace=True)\n",
    "\n",
    "test.rename(columns=nombres, inplace=True)\n",
    "test.drop(['ss_name', 'es_name', 'z_code', 'e_date','es_id'], axis=1, inplace=True)\n",
    "test.rename(columns={'id':'id_final'}, inplace=True)\n",
    "test.rename(columns={'sid':'id'}, inplace=True)\n",
    "test['s_date'] = pd.to_datetime(test['s_date'], format='%m/%d/%Y %H:%M')\n",
    "test['year'] = test['s_date'].map(lambda x:x.year)\n",
    "test['month'] = test['s_date'].map(lambda x:x.month)\n",
    "test['day'] = test['s_date'].map(lambda x:x.day)\n",
    "test['weekday'] = test['s_date'].map(lambda x: x.weekday())\n",
    "test['hour'] = test['s_date'].map(lambda x:x.hour)\n",
    "test['minute'] = test['s_date'].map(lambda x:x.minute)\n",
    "test['date'] = pd.DatetimeIndex(test['s_date']).normalize()\n",
    "test = test.merge(pd.get_dummies(test.sub_type), left_index=True, right_index=True)\n",
    "test.drop('sub_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[train.duration <= 21600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stations = pd.read_csv('./data/station.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations.drop(['name','lat','long','installation_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('./data/weather.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip_ciudad(zip_code):\n",
    "    if zip_code == 95113: return 'San Jose'\n",
    "    elif zip_code == 94301: return 'Palo Alto'\n",
    "    elif zip_code == 94107: return 'San Francisco'\n",
    "    elif zip_code == 94063: return 'Redwood City'\n",
    "    else: return 'Mountain View'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename = {'max_temperature_f': 'max_temp', \n",
    "          'mean_temperature_f': 'mean_temp', \n",
    "          'min_temperature_f': 'min_temp', \n",
    "          'max_dew_point_f': 'max_dew', \n",
    "          'mean_dew_point_f': 'mean_dew', \n",
    "          'min_dew_point_f': 'min_dew', \n",
    "          'max_humidity': 'max_hum', \n",
    "          'mean_humidity': 'mean_hum', \n",
    "          'min_humidity': 'min_hum', \n",
    "          'max_sea_level_pressure_inches': 'max_slp', \n",
    "          'mean_sea_level_pressure_inches': 'mean_slp', \n",
    "          'min_sea_level_pressure_inches': 'min_slp', \n",
    "          'max_visibility_miles': 'max_vis', \n",
    "          'mean_visibility_miles': 'mean_vis', \n",
    "          'min_visibility_miles': 'min_vis', \n",
    "          'max_wind_Speed_mph': 'max_wind', \n",
    "          'mean_wind_speed_mph': 'mean_wind', \n",
    "          'max_gust_speed_mph': 'max_gust', \n",
    "          'precipitation_inches': 'precipitation', \n",
    "          'cloud_cover': 'cloud', \n",
    "          'wind_dir_degrees': 'wind_dir', \n",
    "          'zip_code': 'z_code'\n",
    "         }\n",
    "\n",
    "weather.rename(columns=rename, inplace=True)\n",
    "\n",
    "weather['date'] = pd.to_datetime(weather['date'], format='%m/%d/%Y')\n",
    "weather.loc[weather.events == 'rain', 'events'] = 'Rain'\n",
    "weather.loc[weather.events.isnull(), 'events'] = 'Normal'\n",
    "weather = weather.merge(pd.get_dummies(weather.events), left_index=True, right_index=True)\n",
    "\n",
    "weather.drop(['precipitation'], axis=1, inplace=True)\n",
    "\n",
    "weather['city'] = weather['z_code'].map(zip_ciudad)\n",
    "\n",
    "for i in weather:\n",
    "    weather.drop(weather.loc[weather[i].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(train, stations, how='inner', on='id')\n",
    "test_final = pd.merge(test, stations, how='inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, weather, how='inner', on=['date','city'])\n",
    "test_final = pd.merge(test_final, weather, how='inner', on=['date','city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(pd.get_dummies(df.city), left_index=True, right_index=True)\n",
    "test_final = test_final.merge(pd.get_dummies(test_final.city), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['s_date','date','events','z_code','city'], axis=1, inplace=True)\n",
    "test_final.drop(['s_date','date','events','z_code','city'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "Y = df.duration.values\n",
    "df = normalize(df.drop(['duration'], axis=1))\n",
    "ids_final = test_final.id_final.values\n",
    "test_final = normalize(test_final.drop(['id_final'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.ensemble import * \n",
    "from sklearn.tree import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.kernel_ridge import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores_de_predictores_CV_5():\n",
    "    for reg in regs:\n",
    "        print regs[reg],np.mean(cross_val_score(reg, x, y, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=1) * -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regs = {AdaBoostRegressor():         'AdaBoost         ',\n",
    "        BaggingRegressor():          'Bagging          ',\n",
    "        ExtraTreesRegressor():       'ExtraTrees       ',\n",
    "        GradientBoostingRegressor(): 'GrandientBoosting',\n",
    "        RandomForestRegressor():     'RandomForest     ',\n",
    "        DecisionTreeRegressor():     'DecisionTree     ',\n",
    "        HuberRegressor():            'Huber            ',\n",
    "        LinearRegression():          'Linear           ',\n",
    "        LogisticRegression():        'Logistic         ',\n",
    "        PassiveAggressiveRegressor():'PassiveAggressive',\n",
    "        RANSACRegressor():           'RANSAC           ',\n",
    "        SGDRegressor():              'SGD              ',\n",
    "        TheilSenRegressor():         'TheilSen         ',\n",
    "        KernelRidge():               'KernelRidge      ',\n",
    "        GaussianProcessRegressor():  'GaussianProcess  ',\n",
    "        KNeighborsRegressor():       'KNN              ',\n",
    "        RadiusNeighborsRegressor():  'RNN              ',\n",
    "        MLPRegressor():              'MLP              ',\n",
    "        PLSRegression():             'PLS              ',\n",
    "        SVR():                       'SVR              ',\n",
    "        ElasticNet():                'ElasticNet       ',\n",
    "        BayesianRidge():             'BayesRidge       ',\n",
    "        LarsCV():                    'LarsCV           ',\n",
    "        RidgeCV():                   'RidgeCV          '\n",
    "       }\n",
    "\n",
    "def plots_de_predictores_3000():\n",
    "    x = df[:3000]\n",
    "    y = Y[:3000]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "    plt.figure(figsize=(40, 40));\n",
    "    i = 1\n",
    "    for reg in regs:\n",
    "        s1 = time.time()\n",
    "        reg.fit(X_train, y_train)\n",
    "        s2 = time.time()\n",
    "        pred = reg.predict(X_test)\n",
    "        score = mse(pred, y_test)\n",
    "        plt.subplot(6, 4, i)\n",
    "        plt.title(regs[reg] + str(s2-s1))\n",
    "        plt.plot(pred)\n",
    "        plt.plot(y_test)\n",
    "        i += 1\n",
    "    print 'fin'\n",
    "\n",
    "#plots_de_predictores_3000()\n",
    "#scores_de_predictores_CV_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seleccionamos los mejores algoritmos y realizamos CV con 100000 registros.\n",
    "def cv_mejores_estimadores_100000():\n",
    "    x = df[:100000]\n",
    "    y = Y[:100000]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    regs = {\n",
    "            BaggingRegressor():          'Bagging          ',\n",
    "            ExtraTreesRegressor():       'ExtraTrees       ',\n",
    "            DecisionTreeRegressor():     'DecisionTree     ',\n",
    "           }\n",
    "\n",
    "    scores_de_predictores_CV_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GridSearch de ExtraTrees.\n",
    "def gs_extra_trees_5000():\n",
    "    x = df[:5000]\n",
    "    y = Y[:5000]\n",
    "    params = {\"n_estimators\": list(range(10,31))}\n",
    "    ext = ExtraTreesRegressor()\n",
    "\n",
    "    clf = GridSearchCV(ext, param_grid=params)\n",
    "    clf.fit(x, y)\n",
    "    print \"Mejores parametros:\"\n",
    "    clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparacion de ExtraTrees con y sin parametros con 100000 registros.\n",
    "def comparacion_extra_trees_100000():\n",
    "    x = df[:100000]\n",
    "    y = Y[:100000]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    regs = {\n",
    "            ExtraTreesRegressor():       'ExtraTrees sin parametros       ',\n",
    "            ExtraTreesRegressor(n_estimators=20):    'ExtraTrees con parametros     ',\n",
    "           }\n",
    "\n",
    "    scores_de_predictores_CV_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def estimacion_final():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, Y, test_size=0)\n",
    "    \n",
    "    #est = ExtraTreesRegressor(n_estimators=29)\n",
    "    est = ExtraTreesRegressor()\n",
    "    est.fit(X_train, y_train)\n",
    "    predicciones = est.predict(test_final)\n",
    "    return predicciones\n",
    "\n",
    "predicciones = estimacion_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.152279854\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116349,), (116349,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_final.shape, predicciones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('submission.csv','w')\n",
    "for i in range(len(ids_final)):\n",
    "    f.write(str(ids_final[i])+','+'%.f'%predicciones[i]+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f=open('sumission.csv','w')\n",
    "for i in predicciones:\n",
    "    f.write('%.f'%(,i)+',\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
